{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<img width='700' src=\"https://user-images.githubusercontent.com/8030363/108961534-b9a66980-7634-11eb-96e2-cc46589dcb8c.png\" style=\"vertical-align:middle\">\n",
    "\n",
    "## Pre-Knowledge Graph Build Ontology Cleaning\n",
    "***\n",
    "***\n",
    "\n",
    "**Author:** [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com)  \n",
    "**GitHub Repository:** [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki)  \n",
    "**Release:** **[v2.0.0](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**\n",
    "  \n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook serves as a script to help prepare ontologies prior to be ingested into the knowledge graph build algorithm. This script performs the following steps:  \n",
    "1. [Clean Ontologies](#clean-ontologies)  \n",
    "2. [Merge Ontologies](#merge-ontologies)  \n",
    "\n",
    "## Assumptions and Dependencies  \n",
    "  \n",
    "**Assumptions:**   \n",
    "- Directory of Imported Ontologies has been populated ➞ `./resources/ontologies`     \n",
    "\n",
    "**Dependencies:**   \n",
    "- <u>Scripts</u>: This notebook utilizes several helper functions from the following scripts:  \n",
    "  - [utility scripts](https://github.com/callahantiff/PheKnowLator/blob/master/pkt_kg/utils)  \n",
    "  - [ontology_cleaning.py](https://github.com/callahantiff/PheKnowLator/blob/master/builds/ontology_cleaning.py) \n",
    "- <u>Software</u>: [OWLTools](https://github.com/owlcollab/owltools)  \n",
    "- <u>Data</u>: [`Merged_gene_rna_protein_identifiers.pkl`](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/Merged_gene_rna_protein_identifiers.pkl), which is automatically downloaded to the `./resources/ontologies` directory     \n",
    "\n",
    "<br>\n",
    "\n",
    "Details on the data utilized in this script can be found on the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki. Data can be downloaded from [this](https://console.cloud.google.com/storage/browser/pheknowlator/release_v2.0.0?project=pheknowlator) dedicated Google Cloud Storage Bucket. Please note that all build data are freely available and organized by release and build date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "### CLEAN ONTOLOGIES <a class=\"anchor\" id=\"clean-ontologies\"></a>\n",
    "***\n",
    "\n",
    "The ontology cleaning step includes the following error checks, each of which are explained below and each of which are applied to individual ontologies, the set of merged ontologies or both: (1) Value Errors, (2) Identifier Errors, (3) Duplicate and Obsolete Entities, (4) Punning Errors, and (5) Entity Normalization Errors.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Value Errors  \n",
    "*** \n",
    "**Level:** `individual ontology`; `merged-ontology`    \n",
    "\n",
    "**Description:** This check utilizes the [`owlready2`](https://pypi.org/project/Owlready2/) library to read in each of the ontologies. This library is strict and will catch a wide variety of value errors. \n",
    "\n",
    "**Solution:** Parse the error message using the provided `ErrorType` and line number and repair it. For `ValueErrors` incorrectly typed input are re-typed.\n",
    "\n",
    "*Example Findings*  \n",
    "The [Cell Line Ontology](http://www.clo-ontology.org/) yield the following error message:\n",
    "\n",
    "```python\n",
    "ValueError: invalid literal for int() with base 10: '永生的乳腺衍生细胞系细胞'\n",
    "...\n",
    "OwlReadyOntologyParsingError: RDF/XML parsing error in file clo_with_imports.owl, line 10970, column 99.\n",
    "```\n",
    "\n",
    "This tells us that we need to repair the triple containing the Literal '永生的乳腺衍生细胞系细胞' by removing it and redefining it as a `string`, rather than an `int` as it is currently defined as. This is currently noted as an issue in the [Cell Line Ontology's](http://www.clo-ontology.org/) GitHub repo ([issue #48](https://github.com/CLO-ontology/CLO/issues/48)). \n",
    "\n",
    "<br>\n",
    "\n",
    "### Identifier Errors  \n",
    "*** \n",
    "**Level:** `individual ontology`; `merged-ontology`  \n",
    "\n",
    "**Description:** This check verifies consistency of identifier prefixes. For example, we want to find identifiers that are incorrectly formatted like occurrences of `PRO_XXXXXXX` which should be `PR_XXXXXXX`.\n",
    "\n",
    "**Solution:** Incorrectly formatted class identifiers are updated. This is a tricky task to do in an automated manner and is something that should be updated if any new ontologies are added to the `PheKnowLator` build. Currently, the code below checks and logs any hits, but only fixes the following known errors: Vaccine Ontology: `PRO` which should be `PR`.\n",
    "\n",
    "*Example Findings*  \n",
    "Running this check revealed mislabeling of `2` [pROtein Ontology](https://proconsortium.org/) identifiers in the [Vaccine Ontology](http://www.violinet.org/vaccineontology/) (see [this](https://github.com/vaccineontology/VO/issues/4) GitHub issue).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Obsolete and/or Deprecated Entities\n",
    "*** \n",
    "**Level:** `individual ontology`; `merged-ontology`  \n",
    "\n",
    "**Description:** Verify that the ontology only contains current content.\n",
    "\n",
    "**Solution:** All obsolete classes and any triples that they participate in are removed from an ontologies.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Normalization Errors  \n",
    "*** \n",
    "**Level:** `merged-ontology`\n",
    "\n",
    "These checks are performed at the individual- and merged-ontology levels. There are two types of checks that are performed:  \n",
    "\n",
    "<u>Normalize Existing Ontology Classes</u>  \n",
    "  - **Description:** Checks for inconsistencies in ontology classes that overlap with non-ontology entity identifiers (e.g. if HP includes `HGNC` identifiers, but PheKnowLator utilizes `Entrez` identifiers). \n",
    "\n",
    "  - **Solution:** While there are other types of identifiers, we currently focus primarily on resolving errors involving the genomic identifiers, since we have a master dictionary we can use([`Merged_gene_rna_protein_identifiers.pkl`](https://storage.googleapis.com/pheknowlator/release_v2.0.0/current_build/data/processed_data/Merged_gene_rna_protein_identifiers.pkl)). This check can be updated in future iterations to include other types of identifiers, but given our detailed examination of the `v2.0.0` ontologies, these were the identifier types that needed repair.\n",
    "\n",
    "<u>Normalize Duplicate Ontology Concepts</u>  \n",
    "  - **Description:** Make sure that all classes that represent the same entity are connected to each other. For example, consider the following: the [Sequence Ontology](http://www.sequenceontology.org/), [ChEBI](https://www.ebi.ac.uk/chebi), and [PRotein Ontology](https://proconsortium.org/) all include terms for protein, but none of these classes are connected to each other.\n",
    "\n",
    "  - **Solution:** Choose a primary concept for all duplicate scenarios and make duplicate concepts an `RDFS:subClassOf` the primary concept. In the future, this check could be improved by leveraging [KBOOM](https://www.biorxiv.org/content/10.1101/048843v3).\n",
    "\n",
    "*Example Findings*  \n",
    "The follow classes occur in all of the ontologies used in the current build and have to be normalized so that there are not multiple versions of the same concept:  \n",
    "\n",
    "- Gene: [VO](http://purl.obolibrary.org/obo/OGG_0000000002)  \n",
    "  - <u>Solution</u>: Make the `VO` imported `OGG` class a subclass of the `SO` gene term  \n",
    "\n",
    "- Protein: [SO](http://purl.obolibrary.org/obo/SO_0000104), [PRO](http://purl.obolibrary.org/obo/PR_000000001), [ChEBI](http://purl.obolibrary.org/obo/CHEBI_36080) \n",
    "  - <u>Solution</u>: Make the `CHEBI` and `PRO` classes a subclass of the `SO` protein term  \n",
    "  \n",
    "- Disorder: [VO](http://purl.obolibrary.org/obo/OGMS_0000045)  \n",
    "  - <u>Solution</u>: Make the `VO` imported `OGMS` class a subclass of the `MONDO` disease term  \n",
    "\n",
    "- Antigen: [VO](http://purl.obolibrary.org/obo/OBI_1110034)  \n",
    "  - <u>Solution</u>: Make the `VO` imported OBI class a subclass of the `CHEBI` antigen term  \n",
    "\n",
    "- Gelatin: [VO]('http://purl.obolibrary.org/obo/VO_0003030') \n",
    "  - <u>Solution</u>: Make the `VO` class a subclass of the `CHEBI` gelatin term \n",
    "\n",
    "- Hormone: [VO](http://purl.obolibrary.org/obo/FMA_12278) \n",
    "  - <u>Solution</u>: Make the `VO` imported `FMA` class a subclass of the `CHEBI` hormone term\n",
    "\n",
    "<br>\n",
    "\n",
    "### Punning Errors \n",
    "*** \n",
    "**Level:** `individual ontology`; `merged-ontology`\n",
    "\n",
    "**Description:** [Punning](https://www.w3.org/2007/OWL/wiki/Punning) or redeclaration errors occur for a few different reasons, but the primary or most prevalent cause observed in the ontologies used in `PheKnowLator` is due to an `owl:ObjectProperty` being incorrectly redeclared as an `owl:AnnotationProperty` or an `owl:Class` also being defined as an `OWL:ObjectProperty`. \n",
    "\n",
    "**Solution:** Consistent with the solution described [here](https://github.com/oborel/obo-relations/issues/130), for `owl:ObjectProperty` redeclarations we remove all `owl:AnnotationProperty` declarations. For all `owl:Class` redeclarations, we remove all `owl:ObjectProperty` redeclarations. \n",
    "\n",
    "*Example Findings* \n",
    "The [Cell Line Ontology](http://www.clo-ontology.org/) had 7 object properties that were illegally redeclared and triggered punning errors. More details regarding these errors are shown below. \n",
    "\n",
    "```bash\n",
    "2020-12-03 20:57:15,616 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002091 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002091>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002091>))]\n",
    "2020-12-03 20:57:15,619 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/BFO_0000062 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/BFO_0000062>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/BFO_0000062>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/BFO_0000063 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/BFO_0000063>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/BFO_0000063>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002222 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002222>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002222>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0000087 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0000087>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0000087>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002161 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002161>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002161>))]\n",
    "```\n",
    "\n",
    "From this message, we can see that we need to remove the following `owl:ObjectProperty` redeclared to `owl:AnnotationProperty`: `RO_0002091`, `BFO_0000062`, `BFO_0000063`, `RO_0002222`, `RO_0000087`, `RO_0002161`. There were also 2 classes (i.e. `CLO_0054407` and `CLO_0054409`) defined as being a `owl:Class` and an `owl:ObjectProperty`. This is currently noted as an issue in the Cell Line Ontology's GitHub repo [issue #43](https://github.com/CLO-ontology/CLO/issues/43))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***  \n",
    "### Set-Up Environment\n",
    "***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Cython>=0.29.14\n",
      "  Downloading Cython-0.29.23-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting more-itertools\n",
      "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 766 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.5)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.19.2)\n",
      "Collecting openpyxl>=3.0.3\n",
      "  Using cached openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.2.1)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.8.0-cp38-cp38-manylinux2010_x86_64.whl (296 kB)\n",
      "\u001b[K     |████████████████████████████████| 296 kB 65.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-json-logger\n",
      "  Using cached python-json-logger-2.0.1.tar.gz (9.1 kB)\n",
      "Collecting ray\n",
      "  Downloading ray-1.4.0-cp38-cp38-manylinux2014_x86_64.whl (49.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 49.2 MB 151 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rdflib in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (4.2.2)\n",
      "Collecting reactome2py\n",
      "  Downloading reactome2py-3.0.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (2.25.1)\n",
      "Collecting responses==0.10.12\n",
      "  Using cached responses-0.10.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (4.56.2)\n",
      "Requirement already satisfied: urllib3 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (1.26.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from networkx->-r requirements.txt (line 3)) (4.4.2)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from pandas>=1.0.5->-r requirements.txt (line 6)) (2020.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from pandas>=1.0.5->-r requirements.txt (line 6)) (2.8.1)\n",
      "Collecting protobuf>=3.15.3\n",
      "  Downloading protobuf-3.17.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 59.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from ray->-r requirements.txt (line 9)) (3.2.0)\n",
      "Collecting opencensus\n",
      "  Using cached opencensus-0.7.13-py2.py3-none-any.whl (127 kB)\n",
      "Collecting aioredis\n",
      "  Using cached aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "\u001b[K     |████████████████████████████████| 662 kB 57.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gpustat\n",
      "  Downloading gpustat-0.6.0.tar.gz (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.2-cp38-cp38-manylinux1_x86_64.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 56.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.7.4.post0-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 57.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic>=1.8\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 55.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py-spy>=0.2.0\n",
      "  Downloading py_spy-0.3.7-py2.py3-none-manylinux1_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 58.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp-cors\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from ray->-r requirements.txt (line 9)) (1.32.0)\n",
      "Collecting redis>=3.5.0\n",
      "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 94 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from ray->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/sanya/.local/lib/python3.8/site-packages (from ray->-r requirements.txt (line 9)) (8.0.1)\n",
      "Requirement already satisfied: isodate in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from rdflib->-r requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: pyparsing in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from rdflib->-r requirements.txt (line 10)) (2.4.7)\n",
      "Requirement already satisfied: json5>=0.8.4 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from reactome2py->-r requirements.txt (line 11)) (0.9.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from requests->-r requirements.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from requests->-r requirements.txt (line 12)) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from requests->-r requirements.txt (line 12)) (2.10)\n",
      "Requirement already satisfied: six in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from responses==0.10.12->-r requirements.txt (line 13)) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from jsonschema->ray->-r requirements.txt (line 9)) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from jsonschema->ray->-r requirements.txt (line 9)) (20.3.0)\n",
      "Requirement already satisfied: setuptools in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from jsonschema->ray->-r requirements.txt (line 9)) (50.3.0.post20201006)\n",
      "Collecting opencensus-context==0.1.2\n",
      "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting google-api-core<2.0.0,>=1.0.0\n",
      "  Downloading google_api_core-1.30.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 278 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting hiredis\n",
      "  Downloading hiredis-2.0.0-cp38-cp38-manylinux2010_x86_64.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 643 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-ml-py3>=7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting blessings>=1.6\n",
      "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 66.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from aiohttp->ray->-r requirements.txt (line 9)) (3.7.4.3)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[K     |████████████████████████████████| 324 kB 57.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2.0dev,>=1.25.0\n",
      "  Downloading google_auth-1.32.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 70.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 68.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray->-r requirements.txt (line 9)) (20.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray->-r requirements.txt (line 9)) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray->-r requirements.txt (line 9)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray->-r requirements.txt (line 9)) (4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray->-r requirements.txt (line 9)) (0.4.8)\n",
      "Building wheels for collected packages: python-json-logger, gpustat, nvidia-ml-py3\n",
      "  Building wheel for python-json-logger (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-json-logger: filename=python_json_logger-2.0.1-py34-none-any.whl size=7374 sha256=f3102f5ce2d404cf723b1526eccb5d2d351154aad1f7291a59bca60f0620c02b\n",
      "  Stored in directory: /home/sanya/.cache/pip/wheels/17/25/e2/bc6585122b6e1b5b2bce42a9756143e0d85c317874abb5623e\n",
      "  Building wheel for gpustat (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=626b6eb081ec72a0dfb2844210a79433afb9a0bd78dbe74bf464b73f3c1b70d8\n",
      "  Stored in directory: /home/sanya/.cache/pip/wheels/0d/d9/80/b6cbcdc9946c7b50ce35441cc9e7d8c5a9d066469ba99bae44\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=947781bd9dcad83a86b964f0b48d0eaf0b94572507892f2bfb2bdd01749bcf2f\n",
      "  Stored in directory: /home/sanya/.cache/pip/wheels/b9/b1/68/cb4feab29709d4155310d29a421389665dcab9eb3b679b527b\n",
      "Successfully built python-json-logger gpustat nvidia-ml-py3\n",
      "Installing collected packages: Cython, more-itertools, et-xmlfile, openpyxl, psutil, python-json-logger, protobuf, opencensus-context, google-auth, googleapis-common-protos, google-api-core, opencensus, async-timeout, hiredis, aioredis, pyyaml, nvidia-ml-py3, blessings, gpustat, colorama, msgpack, filelock, multidict, yarl, aiohttp, pydantic, py-spy, aiohttp-cors, redis, ray, reactome2py, responses\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.14.0\n",
      "    Uninstalling protobuf-3.14.0:\n",
      "      Successfully uninstalled protobuf-3.14.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.24.0\n",
      "    Uninstalling google-auth-1.24.0:\n",
      "      Successfully uninstalled google-auth-1.24.0\n",
      "Successfully installed Cython-0.29.23 aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 et-xmlfile-1.1.0 filelock-3.0.12 google-api-core-1.30.0 google-auth-1.32.0 googleapis-common-protos-1.53.0 gpustat-0.6.0 hiredis-2.0.0 more-itertools-8.8.0 msgpack-1.0.2 multidict-5.1.0 nvidia-ml-py3-7.352.0 opencensus-0.7.13 opencensus-context-0.1.2 openpyxl-3.0.7 protobuf-3.17.3 psutil-5.8.0 py-spy-0.3.7 pydantic-1.8.2 python-json-logger-2.0.1 pyyaml-5.4.1 ray-1.4.0 reactome2py-3.0.0 redis-3.5.3 responses-0.10.12 yarl-1.6.3\n"
     ]
    }
   ],
   "source": [
    "# # uncomment and run to install any required modules from notebooks/requirements.txt\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure builds/*.py files and pkt_kg scripts can be reached from notebooks dir\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanya/.conda/envs/sanya_ml/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "from rdflib import Graph\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import script containing helper functions\n",
    "from pkt_kg.utils import * \n",
    "from builds.ontology_cleaning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment variables\n",
    "write_location = '../resources/ontologies'\n",
    "knowledge_graphs_location = '../resources/knowledge_graphs'\n",
    "processed_data_location = '../resources/processed_data/'\n",
    "\n",
    "# set global namespaces\n",
    "schema = Namespace('http://www.w3.org/2001/XMLSchema#')\n",
    "obo = Namespace('http://purl.obolibrary.org/obo/')\n",
    "oboinowl = Namespace('http://www.geneontology.org/formats/oboInOwl#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions needed for processing ontologies\n",
    "def logically_verifies_cleaned_ontologies(graph, temp_dir, file_location, owltools_location):\n",
    "    \"\"\"Logically verifies an ontology by running the ELK deductive logic reasoner. Before running the reasoner\n",
    "    the instantiated RDFLib object is saved locally.\n",
    "\n",
    "    Args:\n",
    "        graph: An RDFLib Graph object containing data.\n",
    "        temp_dir: A string specifying where where to read from and write to.\n",
    "        file_location: The name of the file to read and write to in the temp_dir directory.\n",
    "        owltools_location: A string specifying the location of OWLTOOLs (included in pkt_kg no need to download).\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Logically Verifying Ontology')\n",
    "\n",
    "    # save graph in order to run reasoner\n",
    "    filename = temp_dir + '/' + file_location\n",
    "    graph.serialize(destination=filename, format='xml')\n",
    "    \n",
    "    # run reasoner\n",
    "    command = \"{} {} --reasoner {} --run-reasoner --assert-implied -o {}\"\n",
    "    return_code = os.system(command.format(owltools_location, filename, 'elk', filename))\n",
    "    if return_code != 0: raise ValueError('Reasoner Finished with Errors.')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "### INDIVIDUAL ONTOLOGIES <a class=\"anchor\" id=\"individual-ontologies\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** This section focuses on cleaning the individual ontologies which consists of fixing: (1) Parsing Errors; (2) Identifier Errors; (3) Deprecated and Obsolete Classes; and (4) Punning Errors.\n",
    "\n",
    "\n",
    "**Inputs:** A directory (`write_location`) containing ontology files (`.owl`)\n",
    "\n",
    "**Outputs:** A directory (`write_location`) containing cleaned ontology files (`.owl`)  \n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### ⚡ Important ⚡\n",
    "\n",
    "The `OWL API`, when running the [ELK reasoner](), seems to add back some of the errors that this script removes.\n",
    "\n",
    "- <u>Example 1</u>: In the Vaccine Ontology, we fix prefix errors where `\"PR\"` is recorded as `\"PRO\"`. If you save the ontology without running the reasoner and reload it, the fix remains. If you open it after running ELK, the fix has been reversed. \n",
    "\n",
    "\n",
    "- <u>Example 2</u>: When we create the human subset of the Protein Ontology we verify that it contains only a single large connected component. If you re-calculate the number of connected components after running ELK, there will be three components.  \n",
    "\n",
    "Luckily, the merged ontologies are not logically verified using a reasoner, thus the version used to build knowledge graphs remains free of these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Data from https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/Merged_gene_rna_protein_identifiers.pkl\n"
     ]
    }
   ],
   "source": [
    "# instantiate and set-up class\n",
    "ont_data = OntologyCleaner('', '', '', write_location)\n",
    "\n",
    "# updating ontology info dictionary\n",
    "ont_data.ontology_info = {k.split('/')[-1]: {} for k, v in ont_data.ontology_info.items()}\n",
    "\n",
    "# set owl tools location\n",
    "ont_data.owltools_location = '../pkt_kg/libs/owltools'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chebi-merged-20210624.owl', 'ro_with_imports_AD_mods.owl', 'pr_with_imports.owl', 'chebi_lite_with_imports.owl', 'po_with_imports.owl', 'mondo_with_imports.owl', 'pw_with_imports.owl', 'go_with_imports.owl'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ont_data.ontology_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../resources/ontologies'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ont_data.temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Processing Ontology: CHEBI-MERGED-20210624.OWL ####\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1397913/1397913 [00:56<00:00, 24706.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Finding Parsing Errors\n",
      "Fixing Identifier Errors\n",
      "Removing Deprecated and Obsolete Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 1/1397913 [00:00<60:35:01,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving Punning Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1397913/1397913 [00:07<00:00, 197195.15it/s]\n",
      "100%|██████████| 223111/223111 [00:06<00:00, 33663.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logically Verifying Ontology\n",
      "Reading in Cleaned Ontology -- Needed to Calculate Final Statistics\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1397913/1397913 [00:54<00:00, 25435.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "\n",
      "#### Processing Ontology: RO_WITH_IMPORTS_AD_MODS.OWL ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 3751/7687 [00:00<00:00, 37497.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7687/7687 [00:00<00:00, 39164.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Finding Parsing Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * WARNING: ObjectProperty http://purl.obolibrary.org/obo/RO_0003002 belongs to more than one entity types: [owl.ObjectProperty, obo.RO_0002449]; I'm trying to fix it...\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 7687/7687 [00:00<00:00, 246610.54it/s]\n",
      "100%|██████████| 1508/1508 [00:00<00:00, 31847.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing Identifier Errors\n",
      "Removing Deprecated and Obsolete Classes\n",
      "Resolving Punning Errors\n",
      "Logically Verifying Ontology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in Cleaned Ontology -- Needed to Calculate Final Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3284/7687 [00:00<00:00, 32833.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7687/7687 [00:00<00:00, 35414.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "\n",
      "#### Processing Ontology: PR_WITH_IMPORTS.OWL ####\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12271688/12271688 [08:15<00:00, 24753.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Finding Parsing Errors\n",
      "Fixing Identifier Errors\n",
      "Removing Deprecated and Obsolete Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7641/7641 [00:04<00:00, 1626.97it/s]\n",
      "  0%|          | 1/12221140 [00:01<4447:44:48,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving Punning Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12221140/12221140 [01:04<00:00, 190115.58it/s]\n",
      "100%|██████████| 2348725/2348725 [01:07<00:00, 34577.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logically Verifying Ontology\n",
      "Reading in Cleaned Ontology -- Needed to Calculate Final Statistics\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12221573/12221573 [09:12<00:00, 22115.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "\n",
      "#### Processing Ontology: CHEBI_LITE_WITH_IMPORTS.OWL ####\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1364874/1364874 [01:12<00:00, 18715.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Finding Parsing Errors\n",
      "Fixing Identifier Errors\n",
      "Removing Deprecated and Obsolete Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18472/18472 [00:04<00:00, 3815.84it/s]\n",
      "  0%|          | 3470/1290926 [00:00<00:37, 34696.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving Punning Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1290926/1290926 [00:06<00:00, 203668.98it/s]\n",
      "100%|██████████| 209252/209252 [00:05<00:00, 35069.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logically Verifying Ontology\n",
      "Reading in Cleaned Ontology -- Needed to Calculate Final Statistics\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1290926/1290926 [00:50<00:00, 25768.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "\n",
      "#### Processing Ontology: PO_WITH_IMPORTS.OWL ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2515/60627 [00:00<00:02, 25141.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60627/60627 [00:01<00:00, 34031.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Finding Parsing Errors\n",
      "Fixing Identifier Errors\n",
      "Removing Deprecated and Obsolete Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 455/455 [00:00<00:00, 1387.09it/s]\n",
      " 39%|███▊      | 21915/56659 [00:00<00:00, 219138.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving Punning Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56659/56659 [00:00<00:00, 227600.60it/s]\n",
      "100%|██████████| 8595/8595 [00:00<00:00, 34206.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logically Verifying Ontology\n",
      "Reading in Cleaned Ontology -- Needed to Calculate Final Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2133/56659 [00:00<00:02, 21324.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56659/56659 [00:01<00:00, 32776.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "\n",
      "#### Processing Ontology: MONDO_WITH_IMPORTS.OWL ####\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2313343/2313343 [01:26<00:00, 26720.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Finding Parsing Errors\n",
      "Fixing Identifier Errors\n",
      "Removing Deprecated and Obsolete Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2552/2552 [00:02<00:00, 929.63it/s] \n",
      "  0%|          | 1/2276618 [00:00<134:23:50,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving Punning Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276618/2276618 [00:11<00:00, 194013.85it/s]\n",
      "100%|██████████| 388206/388206 [00:10<00:00, 35687.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logically Verifying Ontology\n",
      "Reading in Cleaned Ontology -- Needed to Calculate Final Statistics\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2277245/2277245 [01:25<00:00, 26745.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "\n",
      "#### Processing Ontology: PW_WITH_IMPORTS.OWL ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2609/35291 [00:00<00:01, 26081.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [00:01<00:00, 31093.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Finding Parsing Errors\n",
      "Fixing Identifier Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 1705.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Deprecated and Obsolete Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 34901/34901 [00:00<00:00, 230014.19it/s]\n",
      "  0%|          | 0/5065 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving Punning Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5065/5065 [00:00<00:00, 37102.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logically Verifying Ontology\n",
      "Reading in Cleaned Ontology -- Needed to Calculate Final Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2298/34901 [00:00<00:01, 22974.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34901/34901 [00:01<00:00, 29594.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "# clean data\n",
    "for ont in ont_data.ontology_info.keys():\n",
    "    print('\\n#### Processing Ontology: {} ####'.format(ont.upper()))\n",
    "    ont_data.ont_file_location = ont\n",
    "    try:\n",
    "        ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "    except rdflib.exceptions.ParserError as e:\n",
    "        command = \"sed -i 's/_:genid/_genid/g' {}\"\n",
    "        return_code = os.system(command.format(ont_data.temp_dir + '/' + ont_data.ont_file_location))\n",
    "        ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "    # get starting statistics\n",
    "    ont_data.updates_ontology_reporter()\n",
    "    \n",
    "    # clean ontologies\n",
    "    ont_data.fixes_ontology_parsing_errors()\n",
    "    ont_data.fixes_identifier_errors()\n",
    "    ont_data.removes_deprecated_obsolete_entities()\n",
    "    ont_data.fixes_punning_errors()\n",
    "    \n",
    "    # run cleaned ontology through the elk reasoner\n",
    "    logically_verifies_cleaned_ontologies(ont_data.ont_graph,\n",
    "                                          ont_data.temp_dir,\n",
    "                                          ont_data.ont_file_location,\n",
    "                                          ont_data.owltools_location)\n",
    "\n",
    "    # verifies no errors caused during cleaning\n",
    "#     ontology_file_formatter(ont_data.temp_dir, '/' + ont_data.ont_file_location, ont_data.owltools_location)\n",
    "    \n",
    "    # read in cleaned, verified, and updated ontology containing inference\n",
    "    print('Reading in Cleaned Ontology -- Needed to Calculate Final Statistics')\n",
    "    try:\n",
    "        ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "    except rdflib.exceptions.ParserError as e:\n",
    "        command = \"sed -i 's/_:genid/_genid/g' {}\"\n",
    "        return_code = os.system(command.format(ont_data.temp_dir + '/' + ont_data.ont_file_location))\n",
    "        ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "    # get finishing statistics\n",
    "    ont_data.updates_ontology_reporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Processing Ontology: GO_WITH_IMPORTS.OWL ####\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1337562/1337562 [00:54<00:00, 24673.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Finding Parsing Errors\n",
      "Fixing Identifier Errors\n",
      "Removing Deprecated and Obsolete Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 1/1337562 [00:00<55:47:42,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving Punning Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1337562/1337562 [00:07<00:00, 189905.19it/s]\n",
      "100%|██████████| 234160/234160 [00:06<00:00, 34880.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logically Verifying Ontology\n",
      "Reading in Cleaned Ontology -- Needed to Calculate Final Statistics\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1337562/1337562 [01:01<00:00, 21597.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n"
     ]
    }
   ],
   "source": [
    "#Adding gene ontology to merged ontology\n",
    "ont = 'go_with_imports.owl'\n",
    "print('\\n#### Processing Ontology: {} ####'.format(ont.upper()))\n",
    "ont_data.ont_file_location = ont\n",
    "ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "\n",
    "# get starting statistics\n",
    "ont_data.updates_ontology_reporter()\n",
    "\n",
    "# clean ontologies\n",
    "ont_data.fixes_ontology_parsing_errors()\n",
    "ont_data.fixes_identifier_errors()\n",
    "ont_data.removes_deprecated_obsolete_entities()\n",
    "ont_data.fixes_punning_errors()\n",
    "\n",
    "# run cleaned ontology through the elk reasoner\n",
    "logically_verifies_cleaned_ontologies(ont_data.ont_graph,\n",
    "                                      ont_data.temp_dir,\n",
    "                                      ont_data.ont_file_location,\n",
    "                                      ont_data.owltools_location)\n",
    "\n",
    "# verifies no errors caused during cleaning\n",
    "#     ontology_file_formatter(ont_data.temp_dir, '/' + ont_data.ont_file_location, ont_data.owltools_location)\n",
    "\n",
    "# read in cleaned, verified, and updated ontology containing inference\n",
    "print('Reading in Cleaned Ontology -- Needed to Calculate Final Statistics')\n",
    "ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "# get finishing statistics\n",
    "ont_data.updates_ontology_reporter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "### MERGED ONTOLOGIES <a class=\"anchor\" id=\"merge-ontologies\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** In this step, the [OWLTools](https://github.com/owlcollab/owltools) library is used to merge the directory of cleaned ontology files into a single ontology file. Then, the following cleaning steps are performed: (1) Identifier Errors; (2) Duplicate Classes; (3) Duplicate Class Concepts; and (4) Punning Errors.  \n",
    "\n",
    "**Inputs:** A directory of ontology files (`.owl`)\n",
    "\n",
    "**Outputs:** `PheKnowLator_MergedOntologies.owl`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Clean Ontology Data\n",
      "Merging Ontologies: pw_with_imports.owl, mondo_with_imports.owl\n",
      "Merging Ontologies: po_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "Merging Ontologies: chebi_lite_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "Merging Ontologies: pr_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "Merging Ontologies: ro_with_imports_AD_mods.owl, PheKnowLator_MergedOntologies.owl\n",
      "Merging Ontologies: chebi-merged-20210624.owl, PheKnowLator_MergedOntologies.owl\n"
     ]
    }
   ],
   "source": [
    "print('Merge Clean Ontology Data')\n",
    "ont_data.ont_file_location = ont_data.merged_ontology_filename\n",
    "\n",
    "# reorder list of ontology files to prepare for merging\n",
    "onts = [ont_data.temp_dir + '/' + x for x in list(ont_data.ontology_info.keys())\n",
    "        if x != ont_data.merged_ontology_filename]\n",
    "\n",
    "# merge ontologies\n",
    "merges_ontologies(onts, ont_data.temp_dir + '/', ont_data.ont_file_location, ont_data.owltools_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../resources/ontologies'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ont_data.temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load merged ontology and add GO\n",
    "import subprocess\n",
    "owltools = ont_data.owltools_location\n",
    "ont1 = ont_data.temp_dir + '/go_with_imports.owl'\n",
    "ont2 = '../resources/knowledge_graphs/' + ont_data.merged_ontology_filename\n",
    "loc = ont_data.temp_dir + '/' + ont_data.merged_ontology_filename\n",
    "try:\n",
    "    subprocess.check_call([owltools, str(ont1), str(ont2), '--merge-support-ontologies', '-o', loc])\n",
    "except subprocess.CalledProcessError as error: print(error.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Merged Ontology Data\n",
      "Graph Stats: 17274809 triples, 6209652 nodes, 145 predicates, 693102 classes, 19 individuals, 691 object props, 259 annotation props\n"
     ]
    }
   ],
   "source": [
    "# load merged ontologies into RDF Lib Graph object\n",
    "print('Loading Merged Ontology Data')\n",
    "ont_data.ont_graph = Graph().parse(ont_data.temp_dir + '/' + ont_data.ont_file_location)\n",
    "\n",
    "#command = \"sed -i 's/_:genid/_genid/g' {}\"\n",
    "\n",
    "# add merged ontology to dict\n",
    "ont_data.ontology_info[ont_data.ont_file_location] = {}\n",
    "\n",
    "# get stats on merged ontologies\n",
    "print(derives_graph_statistics(ont_data.ont_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Merged Ontologies\n",
    "🤔 *IMPORTANT*🤔  Please note there are a few decisions that can made be made at this point that you may want to consider. For our monthly `PheKnowLator` builds, we prefer to use Entrez gene identifiers. If you have run the [`Data_Preparation.ipynb`](https://github.com/callahantiff/PheKnowLator/blob/master/notebooks/Data_Preparation.ipynb) Jupyter Notebook without makeing updates, you would have also committed yourself to using this type of gene identifier. If you have not done with this and do not want to use Entrez gene, but rather prefer to use what the ontologies provide, please comment out `ont_data.normalizes_existing_classes()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17274809/17274809 [17:22<00:00, 16566.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n",
      "Fixing Identifier Errors\n",
      "Normalizing Duplicate Concepts\n",
      "Normalizing Existing Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 1/17274809 [00:02<11889:10:46,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving Punning Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17274809/17274809 [01:37<00:00, 176491.03it/s]\n",
      "100%|██████████| 3190538/3190538 [01:40<00:00, 31807.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Stats: 17274809 triples, 6209652 nodes, 145 predicates, 693102 classes, 19 individuals, 691 object props, 259 annotation props\n",
      "Obtaining Ontology Statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17274809/17274809 [15:20<00:00, 18767.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Connected Components\n"
     ]
    }
   ],
   "source": [
    "# get starting statistics\n",
    "ont_data.updates_ontology_reporter()\n",
    "\n",
    "# clean merged ontologies\n",
    "ont_data.fixes_identifier_errors()\n",
    "ont_data.normalizes_duplicate_classes()\n",
    "ont_data.normalizes_existing_classes()\n",
    "ont_data.fixes_punning_errors()\n",
    "\n",
    "# get finishing statistics\n",
    "print(derives_graph_statistics(ont_data.ont_graph))\n",
    "ont_data.updates_ontology_reporter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output and Save Results\n",
    "The cleaned merged ontology file is saved to the `resources/knowledge_graphs` directory where it can be detected by the `PheKnowLator` algorithm during the build process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save and Format Merged Ontology Data\n",
      "Applying OWL API Formatting to Knowledge Graph OWL File\n"
     ]
    }
   ],
   "source": [
    "print('Save and Format Merged Ontology Data')\n",
    "ont_data.ont_graph.serialize(destination=knowledge_graphs_location + '/' + ont_data.ont_file_location, format='xml')\n",
    "ontology_file_formatter(knowledge_graphs_location, '/' + ont_data.ont_file_location, ont_data.owltools_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Ontology Cleaning Results  \n",
    "To view the results of the ontology cleaning process print the `ont_data.ontology_info` dictionary. This dictionary is keyed by ontology filename and contains a separate dictionary for each ontology with descriptions of the results for each error check that is performed at the individual- and merged-ontology level. The results are also saved to `resources/ontologies/ontology_cleaning_report.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Starting Statistics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a349f6eeeae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'Original GCS URL'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t- Original GCS URL: {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Original GCS URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'Processed GCS URL'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t- Processed GCS URL: {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Processed GCS URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t- Statistics Before Cleaning:\\n\\t\\t- {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Starting Statistics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t- Statistics After Cleaning:\\n\\t\\t- {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Final Statistics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'ValueErrors'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Starting Statistics'"
     ]
    }
   ],
   "source": [
    "# save output locally\n",
    "ont_order = sorted([x for x in ont_data.ontology_info.keys() if not x.startswith('Phe')]) + [ont_data.ont_file_location]\n",
    "with open(ont_data.temp_dir + '/ontology_cleaning_report.txt', 'w') as o:\n",
    "    o.write('=' * 50 + '\\n{}'.format('ONTOLOGY CLEANING REPORT'))\n",
    "    o.write('\\n{}\\n'.format(str(datetime.datetime.utcnow().strftime('%a %b %d %X UTC %Y'))) + '=' * 50 + '\\n\\n')\n",
    "    for key in ont_order:\n",
    "        o.write('\\n\\n\\nONTOLOGY: {}\\n'.format(key)); o.write('*' * (len(key) + 10) + '\\n\\n')\n",
    "        x = ont_data.ontology_info[key]\n",
    "        if 'Original GCS URL' in x.keys(): o.write('\\t- Original GCS URL: {}\\n'.format(x['Original GCS URL']))\n",
    "        if 'Processed GCS URL' in x: o.write('\\t- Processed GCS URL: {}\\n'.format(x['Processed GCS URL']))\n",
    "        o.write('\\t- Statistics Before Cleaning:\\n\\t\\t- {}\\n'.format(x['Starting Statistics']))\n",
    "        o.write('\\t- Statistics After Cleaning:\\n\\t\\t- {}\\n'.format(x['Final Statistics']))\n",
    "        if 'ValueErrors' in x.keys():\n",
    "            if isinstance( x['ValueErrors'], str): o.write('\\t- Value Errors (n=1):\\n\\t\\t- {}\\n'.format(x['ValueErrors']))\n",
    "            else:\n",
    "                for i in x['ValueErrors']: o.write('\\t\\t- {}\\n'.format(str(i)))\n",
    "        else: o.write('\\t- Value Errors: 0\\n')     \n",
    "        if x['IdentifierErrors'] != 'None':\n",
    "            o.write('\\t- Identifier Errors (n={}):\\n'.format(len(x['IdentifierErrors'].split(', '))))\n",
    "            for i in x['IdentifierErrors'].split(', '): o.write('\\t\\t- {}\\n'.format(str(i)))\n",
    "        else: o.write('\\t- Identifier Errors: 0\\n')\n",
    "        if 'PheKnowLator_MergedOntologies' not in key:\n",
    "            if x['Deprecated'] != 'None':\n",
    "                o.write('\\t- Deprecated Classes (n={}):\\n'.format(len(x['Deprecated'])))\n",
    "                for i in x['Deprecated']: o.write('\\t\\t- {}\\n'.format(str(i)))\n",
    "            else: o.write('\\t- Deprecated Classes: 0\\n') \n",
    "            if x['Obsolete'] != 'None':\n",
    "                o.write('\\t- Obsolete Classes (n={}):\\n'.format(len(x['Obsolete'])))\n",
    "                for i in x['Obsolete']: o.write('\\t\\t- {}\\n'.format(str(i)))\n",
    "            else: o.write('\\t- Obsolete Classes: 0\\n')\n",
    "        o.write('\\t- Punning Errors:\\n')\n",
    "        if x['PunningErrors - Classes'] != 'None':\n",
    "            o.write('\\t\\t- Classes (n={}):\\n'.format(len(x['PunningErrors - Classes'].split(', '))))\n",
    "            for i in x['PunningErrors - Classes'].split(', '): o.write('\\t\\t\\t- {}\\n'.format(i))\n",
    "        else: o.write('\\t\\t- Classes: 0\\n')\n",
    "        if x['PunningErrors - ObjectProperty'] != 'None':\n",
    "            o.write('\\t\\t- Object Properties (n={}):\\n'.format(len(x['PunningErrors - ObjectProperty'].split(', '))))\n",
    "            for i in x['PunningErrors - ObjectProperty'].split(', '): o.write('\\t\\t\\t- {}\\n'.format(i))\n",
    "        else: o.write('\\t\\t- Object Properties: 0\\n')\n",
    "        if 'Normalized - Duplicates' in x.keys():\n",
    "            o.write('\\t- Normalization:\\n')\n",
    "            if x['Normalized - Duplicates'] != 'None':\n",
    "                o.write('\\t\\t- Existing Entity Normalization (n={}):\\n'.format(len(x['Normalized - Duplicates'].split(', '))))\n",
    "                for i in x['Normalized - Duplicates'].split(', '): o.write('\\t\\t\\t- {}\\n'.format(i))\n",
    "            else: o.write('\\t\\t- Entity Normalization: 0\\n')\n",
    "            if x['Normalized - Gene IDs'] != 'None': o.write('\\t\\t- Normalized HGNC IDs: {}\\n'.format(x['Normalized - Gene IDs']))\n",
    "            if x['Normalized - NonOnt'] != 'None': o.write('\\t\\t- Other Classes that May Need Normalization: {}\\n'.format(x['Normalized - NonOnt']))\n",
    "            if x['Normalized - Dep'] != 'None':\n",
    "                o.write('\\t\\t- Deprecated Ontology HGNC Identifiers Needing Alignment (n={}):\\n'.format(len(x['Normalized - Dep'])))\n",
    "                for i in x['Normalized - Dep']: o.write('\\t\\t- {}\\n'.format(i))\n",
    "            else: o.write('\\t\\t- Deprecated Ontology HGNC Identifiers Needing Alignment: 0\\n')\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean-Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove temp file in resources/ontologies\n",
    "os.remove(write_location + '/' + ont_data.ont_file_location)\n",
    "os.remove(write_location + '/Merged_gene_rna_protein_identifiers.pkl')\n",
    "\n",
    "# # remove logs directory\n",
    "# logs = glob.glob('..builds/logs/*.log')\n",
    "# shutil.rmtree('/'.join(logs[0].split('/')[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "```\n",
    "@misc{callahan_tj_2019_3401437,\n",
    "  author       = {Callahan, TJ},\n",
    "  title        = {PheKnowLator},\n",
    "  month        = mar,\n",
    "  year         = 2019,\n",
    "  doi          = {10.5281/zenodo.3401437},\n",
    "  url          = {https://doi.org/10.5281/zenodo.3401437}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
